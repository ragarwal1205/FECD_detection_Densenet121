import torch
import torch.nn as nn
import torch.optim as optim
import torchvision
from torchvision import transforms, models
from torch.optim.lr_scheduler import ExponentialLR
import pandas as pd
from PIL import Image
from sklearn.metrics import classification_report
import numpy as np

# Define data directories (CSV file paths)
train_csv_path = (insert CSV file path) 
valid_csv_path = (insert CSV file path)

# Read CSV files to get image file paths and labels
train_df = pd.read_csv(train_csv_path, dtype=str)
valid_df = pd.read_csv(valid_csv_path, dtype=str)

# Define data transformations
data_transforms = {
    'train': transforms.Compose([
        transforms.RandomResizedCrop(224),
        transforms.RandomHorizontalFlip(),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
    ]),
    'val': transforms.Compose([
        transforms.Resize(256),
        transforms.CenterCrop(224),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
    ]),
}

"""# Create custom dataset classes to load images using file paths
class CustomDataset(torch.utils.data.Dataset):
    def __init__(self, dataframe, transform=None):
        self.dataframe = dataframe
        self.transform = transform

    def __len__(self):
        return len(self.dataframe)

    def __getitem__(self, idx):
        img_name = self.dataframe.loc[idx, 'FilePath']
        try:
            image = Image.open(img_name)
            if self.transform:
                image = self.transform(image)
            label = int(self.dataframe.loc[idx, 'grade'])
            return image, label
        except Exception as e:
            print(f"Error loading image at index {idx}: {e}")
            # Return dummy tensors to indicate loading error
            return torch.zeros(3, 224, 224), -1  # Adjust size and label as needed"""

#To report how many images have been successfully loaded 
class CustomDataset(torch.utils.data.Dataset):
    def __init__(self, dataframe, transform=None):
        self.dataframe = dataframe
        self.transform = transform
        self.successful_loads = 0
        self.failed_loads = 0

    def __len__(self):
        return len(self.dataframe)

    def __getitem__(self, idx):
        img_name = self.dataframe.loc[idx, 'FilePath with Extension']
        try:
            image = Image.open(img_name)
            if self.transform:
                image = self.transform(image)
            label = int(self.dataframe.loc[idx, 'grade'])
            self.successful_loads += 1  # Increment successful loads counter
            return image, label
        except Exception as e:
            print(f"Error loading image at index {idx}: {e}")
            self.failed_loads += 1  # Increment failed loads counter
            # Return dummy tensors to indicate loading error
            return torch.zeros(3, 224, 224), -1  # Adjust size and label as needed

    def report_load_statistics(self):
        print(f"Successfully loaded {self.successful_loads} images.")
        print(f"Failed to load {self.failed_loads} images.")


# Load the datasets using custom dataset classes
image_datasets = {
    'train': CustomDataset(train_df, transform=data_transforms['train']),
    'val': CustomDataset(valid_df, transform=data_transforms['val'])
}

# Create data loaders
dataloaders = {
    'train': torch.utils.data.DataLoader(image_datasets['train'], batch_size=4, shuffle=True, num_workers=8, pin_memory=True),
    'val': torch.utils.data.DataLoader(image_datasets['val'], batch_size=4, shuffle=False, num_workers=8, pin_memory=True)
}

# Define the DenseNet-121 Model without pre-trained weights
model = models.densenet121(weights=models.DenseNet121_Weights.DEFAULT)

# Modify the final classification layer
num_classes = len(image_datasets['train'])
model.classifier = nn.Sequential(
    nn.Linear(1024, 512),
    nn.ReLU(),
    nn.Dropout(0.2),
    nn.Linear(512, num_classes)
)

# Define Loss Function and Optimizer
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.01)

#Add a Learning Rate Scheduler 
scheduler = ExponentialLR(optimizer, gamma=0.95)  # Reduces lr by multiplying by 0.95 each epoch

# Define a variable to keep track of the minimum validation loss
valid_loss_min = float("inf")  # Set to positive infinity initially

# Initialize a list to store validation losses
val_losses = []

# Training and validation loop
valid_loss_min = float("inf")
num_epochs = 100
max_epochs_without_improvement = 10 #10-20% of total epochs 
epochs_without_improvement = 0

# Check if GPU is available and set it as the device
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using {device} device.")

# Transfer the model to the configured device
model = model.to(device)

for epoch in range(num_epochs):
    for phase in ['train', 'val']:
        if phase == 'train':
            model.train()
        else:
            model.eval()

        running_loss = 0.0
        running_corrects = 0

        for inputs, labels in dataloaders[phase]:
            if inputs is None or labels is None or -1 in labels:
                continue  # Skip the batch if it contains invalid data
            
            # Move the inputs and labels to the device
            inputs, labels = inputs.to(device), labels.to(device)
            
            optimizer.zero_grad()
            with torch.set_grad_enabled(phase == 'train'):
                outputs = model(inputs)
                loss = criterion(outputs, labels)

                if phase == 'train':
                    loss.backward()
                    optimizer.step()

                _, preds = torch.max(outputs, 1)
                running_loss += loss.item() * inputs.size(0)
                running_corrects += torch.sum(preds == labels.data)
                
        if phase == 'train':
            scheduler.step()  # Update the learning rate

        epoch_loss = running_loss / len(image_datasets[phase])
        epoch_acc = running_corrects.double() / len(image_datasets[phase])

        print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')
        
        if phase == 'val':
            val_losses.append(epoch_loss)
            if epoch_loss < valid_loss_min:
                print(f'Validation loss decreased ({valid_loss_min:.6f} --> {epoch_loss:.6f}). Saving model...')
                torch.save(model.state_dict(), 'your_model_weights2.pth')
                valid_loss_min = epoch_loss
                epochs_without_improvement = 0
            else:
                epochs_without_improvement += 1

            if epochs_without_improvement >= max_epochs_without_improvement:
                print(f'No improvement in validation loss for {max_epochs_without_improvement} epochs. Stopping training.')
                break
        
        image_datasets[phase].report_load_statistics()

# Generate the classification report after training
model.eval()
true_labels = []
predicted_labels = []
for inputs, labels in dataloaders['val']:
    with torch.no_grad():
        outputs = model(inputs)
        _, preds = torch.max(outputs, 1)
        true_labels.extend(labels.numpy())
        predicted_labels.extend(preds.numpy())

true_labels = np.array(true_labels)
predicted_labels = np.array(predicted_labels)
report = classification_report(true_labels, predicted_labels, target_names=image_datasets['val'].dataframe['grade'].unique())

print(report)
